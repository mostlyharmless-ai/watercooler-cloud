[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "watercooler-cloud"
version = "0.1.4-dev"
description = "File-based collaboration protocol for agentic coding projects"
readme = "README.md"
requires-python = ">=3.10"
license = {text = "Apache-2.0"}
authors = [
    {name = "Mostly Harmless AI", email = "hello@watercoolerdev.com"}
]
dependencies = [
    "fastmcp>=2.0",
    "python-ulid>=2.0",
    "GitPython>=3.1.0",
    "pydantic>=2.0",
    "tomli>=2.0;python_version<'3.11'",
    "tomlkit>=0.12",
]
classifiers = [
    "Development Status :: 2 - Pre-Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: Apache Software License",
    "Programming Language :: Python :: 3",
    # Dropping 3.9; project requires 3.10+ due to PEP 604 usage
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Python Modules",
]

[project.urls]
Homepage = "https://github.com/mostlyharmless-ai/watercooler-cloud"
Repository = "https://github.com/mostlyharmless-ai/watercooler-cloud"
Issues = "https://github.com/mostlyharmless-ai/watercooler-cloud/issues"

[project.optional-dependencies]
dev = [
    "pytest>=7.0",
    "mypy>=1.0",
    "jsonschema>=4.0",
]
mcp = [
    "fastmcp>=2.0",
    "python-ulid>=2.0",
]
http = [
    "fastapi>=0.100",
    "uvicorn[standard]>=0.20",
]
# Baseline graph module - LLM-based summarization for knowledge graphs
# Usage: pip install watercooler-cloud[baseline]
baseline = [
    "httpx>=0.25",
]
memory = [
    "tiktoken>=0.5",
    "httpx>=0.25",
    "openai>=1.0",
]
# Local LLM inference - provides OpenAI-compatible server for offline summarization
# Usage: pip install watercooler-cloud[local]
#        python -m watercooler_memory.local_server  (auto-downloads default model)
local = [
    "llama-cpp-python[server]>=0.2.50",
    "huggingface_hub>=0.20",
    "httpx>=0.25",
]
# Full graph building with local models
graph-local = [
    "tiktoken>=0.5",
    "httpx>=0.25",
    "llama-cpp-python[server]>=0.2.50",
]
# Interactive graph visualization for baseline graphs (scripts/visualize_graph.py)
# See docs/visualization.md for usage
visualization = [
    "pyvis>=0.3.0",
    "networkx>=3.0",
]

[tool.setuptools.packages.find]
where = ["src"]

[tool.setuptools.package-data]
watercooler = ["py.typed", "templates/*.md", "templates/*.toml"]
watercooler_mcp = ["scripts/*"]
watercooler_memory = ["py.typed"]

[project.scripts]
watercooler = "watercooler.cli:main"
watercooler-mcp = "watercooler_mcp.server:main"

[tool.pytest.ini_options]
markers = [
    "http: marks tests as requiring HTTP facade dependencies (deselect with '-m \"not http\"')",
    "integration_falkor: marks tests requiring real FalkorDB connection (run with '-m integration_falkor')",
    "integration_leanrag_llm: marks tests requiring LeanRAG with LLM APIs (DeepSeek, GLM embeddings)",
    "integration_graphiti: marks tests requiring Graphiti backend with API key (run with '-m integration_graphiti')",
]
